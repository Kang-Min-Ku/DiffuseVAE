[2024-12-17 13:42:28,256][__main__][INFO] - data:
  root: /workspace/datasets/
  name: cifar10
  image_size: 32
  n_channels: 3
  hflip: true
model:
  enc_block_config: 32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3
  enc_channel_config: 32:64,16:128,8:256,4:256,1:512
  dec_block_config: 1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15
  dec_channel_config: 32:64,16:128,8:256,4:256,1:512
training:
  seed: 0
  fp16: false
  batch_size: 128
  epochs: 500
  log_step: 50
  device: gpu:0
  chkpt_interval: 1
  optimizer: Adam
  lr: 0.0001
  restore_path: ''
  results_dir: /workspace/stage3_NF_prior_vae_chkpts/
  workers: 2
  chkpt_prefix: vae-cifar10
  alpha: 1.0

[2024-12-17 13:42:31,512][__main__][INFO] - Running Trainer with kwargs: {'resume_from_checkpoint': '', 'default_root_dir': '/workspace/stage3_NF_prior_vae_chkpts/', 'max_epochs': 500, 'log_every_n_steps': 50, 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7af1ed767640>], 'gpus': [0], 'plugins': <pytorch_lightning.plugins.training_type.ddp.DDPPlugin object at 0x7af1ed765de0>}
